# Copy to `.env` (gitignored) and adjust values for your environment.
#
# Profile selection
TPA_PROFILE=oss

# User/Group IDs for non-root containers (security best practice)
# Default: your current user. Run `id -u` and `id -g` to get your UID/GID.
UID=1000
GID=1000

# Postgres (PostGIS + pgvector)
TPA_POSTGRES_HOST=tpa-db
TPA_POSTGRES_PORT=5432
TPA_POSTGRES_DB=tpa
TPA_POSTGRES_USER=tpa
TPA_POSTGRES_PASSWORD=tpa

# MinIO (S3-compatible blob store for OSS profile)
TPA_S3_ENDPOINT=http://tpa-minio:9000
TPA_S3_REGION=local
TPA_S3_BUCKET=tpa
TPA_S3_ACCESS_KEY=tpa
TPA_S3_SECRET_KEY=change-me

# API/UI ports (host)
TPA_API_PORT=8000
TPA_UI_PORT=3000
TPA_UI_DEV_PORT=5173

# Observability (Phoenix)
TPA_PHOENIX_PORT=6006

# Redis (background jobs)
TPA_REDIS_URL=redis://tpa-redis:6379/0

# Optional image overrides (default images aim for latest stable)
TPA_MINIO_IMAGE=minio/minio:latest
TPA_MINIO_MC_IMAGE=minio/mc:latest
TPA_REDIS_IMAGE=redis:alpine
TPA_PHOENIX_IMAGE=arizephoenix/phoenix:latest
TPA_VLLM_IMAGE=vllm/vllm-openai:latest
TPA_TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:turing-latest
TPA_PLAYWRIGHT_IMAGE=mcr.microsoft.com/playwright:latest
TPA_UI_NODE_IMAGE=node:alpine
TPA_UI_NGINX_IMAGE=nginx:alpine
TPA_VISION_PYTHON_IMAGE=python:3.13-slim

# Optional tool service URLs (defaults point at compose service names)
TPA_DOCPARSE_BASE_URL=http://tpa-docparse:8084
TPA_DOCPARSE_PROVIDER=docling
TPA_WEB_AUTOMATION_BASE_URL=http://tpa-web-automation:8085
TPA_SEGMENTATION_BASE_URL=http://tpa-segmentation-worker:8086
TPA_VECTORIZE_BASE_URL=http://tpa-vectorization-worker:8087

# Optional host-exposed ports (only used if you enable the corresponding profile)
TPA_DOCPARSE_PORT=8084
TPA_WEB_AUTOMATION_PORT=8085
TPA_SEGMENTATION_PORT=8086
TPA_VECTORIZE_PORT=8087

# Model endpoints (OSS profile; optional if using mock providers)
TPA_LLM_BASE_URL=http://tpa-llm:8000/v1
TPA_VLM_BASE_URL=http://tpa-vlm:8000/v1
TPA_EMBEDDINGS_BASE_URL=http://tpa-embeddings:8080
TPA_RERANKER_BASE_URL=http://tpa-reranker:8080

# Optional: OSS dev "model supervisor" auto-start/stop (single GPU)
# If set, the API will call the supervisor before using LLM/VLM/embeddings/reranker endpoints.
TPA_MODEL_SUPERVISOR_URL=
# Optional shared secret (recommended if you enable the supervisor).
TPA_MODEL_SUPERVISOR_TOKEN=
# Compose project name used by the supervisor to find containers (defaults to `name:` in docker/compose.oss.yml)
TPA_COMPOSE_PROJECT=tpa-oss
TPA_MODEL_SUPERVISOR_ENFORCE_GPU_EXCLUSIVITY=true
TPA_MODEL_SUPERVISOR_READY_TIMEOUT_SECONDS=180
TPA_MODEL_SUPERVISOR_STOP_TIMEOUT_SECONDS=30

# Policy parsing (ingestion)
# When true (recommended), policy clause extraction is only performed via the LLM parse instrument.
# Set to false to allow a fallback "whole-policy-as-one-clause" record when the LLM is unavailable.
TPA_POLICY_PARSE_REQUIRE_LLM=true

# Model IDs + runtime knobs (optional)
# Tip: with a single 32GB GPU, start models on-demand (LLM vs VLM vs embeddings) to avoid VRAM contention.
TPA_LLM_MODEL_ID=openai/gpt-oss-20b
# vLLM defaults target high throughput; for a single-user workbench prefer small batching.
TPA_LLM_VLLM_ARGS=--max-num-seqs 8
TPA_VLM_MODEL_ID=nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-FP8
TPA_VLM_VLLM_ARGS=
TPA_EMBEDDINGS_MODEL_ID=Qwen/Qwen3-Embedding-8B
TPA_RERANKER_MODEL_ID=Qwen/Qwen3-Reranker-4B

# Hugging Face token (only needed for gated models)
HF_TOKEN=
